{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d296b1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from univa.dataset.qwen2vl_dataset import Qwen2VLDataset\n",
    "from univa.models.qwen2p5vl.modeling_univa_qwen2p5vl import UnivaQwen2p5VLForConditionalGeneration\n",
    "\n",
    "from univa.utils.anyres_util import dynamic_resize\n",
    "from univa.utils.prompter import Qwen2VLPrompter\n",
    "from torchvision import transforms\n",
    "from transformers import (\n",
    "    CLIPTextModel,\n",
    "    T5EncoderModel,\n",
    "    CLIPTokenizer,\n",
    "    T5TokenizerFast,\n",
    "    AutoImageProcessor,\n",
    "    PreTrainedTokenizer,\n",
    "    AutoTokenizer,\n",
    "    AutoProcessor, SiglipImageProcessor\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "from univa.dataset.data_collator import DataCollator, pad_list_of_tensors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08995c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_type = \"qwen2p5vl\"\n",
    "data_txt = \"/workspace/UniWorld-V1/training_data/uniworld_removal_dataset/data.txt\"\n",
    "anyres = 'any_1ratio'\n",
    "\n",
    "\n",
    "anchor_pixels = 1024 * 1024\n",
    "\n",
    "\n",
    "resize_lambda = transforms.Lambda(\n",
    "    lambda img: transforms.Resize(\n",
    "        dynamic_resize(\n",
    "            img.shape[1], img.shape[2], anyres, anchor_pixels), \n",
    "            interpolation=transforms.InterpolationMode.BICUBIC\n",
    "        )(img)\n",
    ")\n",
    "transform = transforms.Compose([\n",
    "        resize_lambda,\n",
    "        transforms.Normalize([0.5], [0.5]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "min_pixels, max_pixels = 1048576, 1048576\n",
    "\n",
    "drop_condition_rate = 0.0\n",
    "joint_ref_feature = False\n",
    "mask_weight_type  = \"log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450482da",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_txt, \"r\") as f:\n",
    "    datasets = [line.strip() for line in f.readlines()]\n",
    "\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642bc307",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_lvlm_name_or_path = \"/workspace/UniWorld-V1/model_weight/UniWorld-V1\"\n",
    "pretrained_siglip_name_or_path = \"/workspace/UniWorld-V1/model_weight/siglip2-so400m-patch16-512\"\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    pretrained_lvlm_name_or_path,\n",
    ")\n",
    "lvlm_tokenizer = processor.tokenizer\n",
    "image_processor = processor.image_processor\n",
    "\n",
    "\n",
    "\n",
    "lvlm_model = UnivaQwen2p5VLForConditionalGeneration.from_pretrained(\n",
    "    pretrained_lvlm_name_or_path,\n",
    "    attn_implementation='flash_attention_2',\n",
    ")\n",
    "\n",
    "\n",
    "prompter = Qwen2VLPrompter()\n",
    "\n",
    "\n",
    "\n",
    "siglip_processor = SiglipImageProcessor.from_pretrained(\n",
    "    pretrained_siglip_name_or_path\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a5cb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Qwen2VLDataset(\n",
    "        dataset_type=dataset_type, \n",
    "        data_txt=data_txt,\n",
    "        transform=transform, \n",
    "        tokenizer=lvlm_tokenizer,\n",
    "        prompter=prompter,\n",
    "        image_processor=image_processor,\n",
    "        processor=processor,\n",
    "        min_pixels=min_pixels,\n",
    "        max_pixels=max_pixels,\n",
    "        image_token_length=lvlm_model.config.image_token_length,\n",
    "        only_generated_task=True,\n",
    "        drop_prompt_rate=drop_condition_rate,\n",
    "        joint_ref_feature=joint_ref_feature, \n",
    "        anyres=anyres, \n",
    "        mask_weight_type=mask_weight_type, \n",
    "        siglip_processor=siglip_processor, \n",
    "        ocr_enhancer=False, \n",
    "        random_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fde07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "pin_memory = True\n",
    "num_workers = 16\n",
    "padding_side = \"left\"\n",
    "\n",
    "data_collator = DataCollator(tokenizer=lvlm_tokenizer, padding_side=padding_side)\n",
    "\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    pin_memory=pin_memory,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=data_collator,\n",
    "    prefetch_factor=None if num_workers == 0 else 4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd6bb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a585ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, batch in enumerate(train_dataloader):\n",
    "    prompts = batch[\"prompts\"]\n",
    "    print(len(prompts))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "univa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
