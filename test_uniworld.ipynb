{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    SiglipImageProcessor,\n",
    "    SiglipVisionModel,\n",
    "    T5EncoderModel,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from univa.models.qwen2p5vl.modeling_univa_qwen2p5vl import (\n",
    "    UnivaQwen2p5VLForConditionalGeneration,\n",
    ")\n",
    "from univa.utils.flux_pipeline import FluxPipeline\n",
    "from univa.utils.get_ocr import get_ocr_result\n",
    "from univa.utils.denoiser_prompt_embedding_flux import encode_prompt\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from univa.utils.anyres_util import dynamic_resize, concat_images_adaptive\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "import uuid\n",
    "import base64\n",
    "from typing import Dict\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from pathlib import Path\n",
    "from diffusers.utils import make_image_grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLUX_PATH = Path(\"model_weight/FLUX.1-dev\")\n",
    "SIGLIP_PATH = Path(\"model_weight/siglip2-so400m-patch16-512\")\n",
    "MODEL_PATH = Path(\"model_weight/UniWorld-V1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_models(nf4=True, offload=True):\n",
    "    os.makedirs(\"tmp\", exist_ok=True)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "    )\n",
    "\n",
    "    # Load main model and task head\n",
    "    model = UnivaQwen2p5VLForConditionalGeneration.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "        quantization_config=quantization_config if nf4 else None,\n",
    "    ).to(device)\n",
    "\n",
    "    task_head = nn.Sequential(\n",
    "        nn.Linear(3584, 10240), nn.SiLU(), nn.Dropout(0.3), nn.Linear(10240, 2)\n",
    "    ).to(device)\n",
    "    task_head.load_state_dict(torch.load(MODEL_PATH / \"task_head_final.pt\"))\n",
    "    task_head.eval()\n",
    "\n",
    "    processor = AutoProcessor.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        min_pixels=448 * 448,\n",
    "        max_pixels=448 * 448,\n",
    "    )\n",
    "\n",
    "    if nf4:\n",
    "        text_encoder_2 = T5EncoderModel.from_pretrained(\n",
    "            FLUX_PATH,\n",
    "            subfolder=\"text_encoder_2\",\n",
    "            quantization_config=quantization_config,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "        )\n",
    "        pipe = FluxPipeline.from_pretrained(\n",
    "            FLUX_PATH,\n",
    "            transformer=model.denoise_tower.denoiser,\n",
    "            text_encoder_2=text_encoder_2,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "        ).to(device)\n",
    "    else:\n",
    "        pipe = FluxPipeline.from_pretrained(\n",
    "            FLUX_PATH,\n",
    "            transformer=model.denoise_tower.denoiser,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "        ).to(device)\n",
    "\n",
    "    if offload:\n",
    "        pipe.enable_model_cpu_offload()\n",
    "        pipe.enable_vae_slicing()\n",
    "\n",
    "    tokenizers = [pipe.tokenizer, pipe.tokenizer_2]\n",
    "    text_encoders = [pipe.text_encoder, pipe.text_encoder_2]\n",
    "\n",
    "    siglip_processor = SiglipImageProcessor.from_pretrained(SIGLIP_PATH)\n",
    "    siglip_model = SiglipVisionModel.from_pretrained(\n",
    "        SIGLIP_PATH,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    ).to(device)\n",
    "\n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"task_head\": task_head,\n",
    "        \"processor\": processor,\n",
    "        \"pipe\": pipe,\n",
    "        \"tokenizers\": tokenizers,\n",
    "        \"text_encoders\": text_encoders,\n",
    "        \"siglip_processor\": siglip_processor,\n",
    "        \"siglip_model\": siglip_model,\n",
    "        \"device\": device,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = initialize_models()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from univa.utils.denoiser_prompt_embedding_flux import encode_prompt\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def run_inference(\n",
    "    text,\n",
    "    image1: Image.Image = None,\n",
    "    image2: Image.Image = None,\n",
    "    height=1024,\n",
    "    width=1024,\n",
    "    steps=30,\n",
    "    guidance=4.0,\n",
    "    joint_with_t5=True,\n",
    "    num_imgs=1,\n",
    "    seed=-1,\n",
    "):\n",
    "    convo = []\n",
    "    content = []\n",
    "    image_list: list[Image.Image] = []\n",
    "\n",
    "    if text:\n",
    "        content.append({\"type\": \"text\", \"text\": text})\n",
    "\n",
    "    for image in [image1, image2]:\n",
    "        if image:\n",
    "            content.append(\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"image\": image,\n",
    "                    \"min_pixels\": 448 * 448,\n",
    "                    \"max_pixels\": 448 * 448,\n",
    "                }\n",
    "            )\n",
    "            image_list.append(image)\n",
    "\n",
    "    convo.append({\"role\": \"user\", \"content\": content})\n",
    "\n",
    "    # === Tokenize input ===\n",
    "    chat_text = state[\"processor\"].apply_chat_template(\n",
    "        convo, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    chat_text = \"<|im_end|>\\n\".join(chat_text.split(\"<|im_end|>\\n\")[1:])\n",
    "    image_inputs, video_inputs = process_vision_info(convo)\n",
    "\n",
    "    inputs = state[\"processor\"](\n",
    "        text=[chat_text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(state[\"device\"])\n",
    "\n",
    "    # === SigLIP embedding (optional)\n",
    "    siglip_hs = None\n",
    "    if state[\"siglip_processor\"] and image_list:\n",
    "        tensors = [\n",
    "            state[\"siglip_processor\"]\n",
    "            .preprocess(\n",
    "                image.convert(\"RGB\"),\n",
    "                do_resize=True,\n",
    "                do_convert_rgb=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            .pixel_values.to(state[\"device\"])\n",
    "            for image in image_list\n",
    "        ]\n",
    "        siglip_hs = state[\"siglip_model\"](torch.cat(tensors)).last_hidden_state\n",
    "\n",
    "    # === Main model forward ===\n",
    "    lvlm = state[\"model\"](\n",
    "        inputs.input_ids,\n",
    "        pixel_values=getattr(inputs, \"pixel_values\", None),\n",
    "        attention_mask=inputs.attention_mask,\n",
    "        image_grid_thw=getattr(inputs, \"image_grid_thw\", None),\n",
    "        siglip_hidden_states=siglip_hs,\n",
    "        output_type=\"denoise_embeds\",\n",
    "    )\n",
    "\n",
    "    # === Add T5 if needed ===\n",
    "    prm_embeds, pooled = encode_prompt(\n",
    "        state[\"text_encoders\"],\n",
    "        state[\"tokenizers\"],\n",
    "        text if joint_with_t5 else \"\",\n",
    "        256,\n",
    "        state[\"device\"],\n",
    "        1,\n",
    "    )\n",
    "    prompt_embeds = torch.cat([lvlm, prm_embeds], dim=1) if joint_with_t5 else lvlm\n",
    "\n",
    "    # === Generator and seed ===\n",
    "    if seed == -1:\n",
    "        seed = torch.seed()\n",
    "    generator = torch.Generator(device=state[\"device\"]).manual_seed(seed)\n",
    "\n",
    "    # === Generate image ===\n",
    "    images = state[\"pipe\"](\n",
    "        prompt_embeds=prompt_embeds,\n",
    "        pooled_prompt_embeds=pooled,\n",
    "        height=height,\n",
    "        width=width,\n",
    "        num_inference_steps=steps,\n",
    "        guidance_scale=guidance,\n",
    "        generator=generator,\n",
    "        num_images_per_prompt=num_imgs,\n",
    "    ).images\n",
    "\n",
    "    return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gemini.gemini_clean import GeminiImageCleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_KEY = \"AIzaSyD9faZ4OGTkuHBmFaU-F6gz_6NNYB32IQQ\"\n",
    "\n",
    "\n",
    "gemini_cleaner = GeminiImageCleaner(\n",
    "    api_key=GEMINI_KEY, config_path=\"gemini/gemini_config.yml\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_IMG_FOLDER = Path(\"/root/UniWorld-V1/empty_room\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_paths = list(TEST_IMG_FOLDER.glob(\"*.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_path = TEST_IMG_FOLDER / \"35.jpg\"\n",
    "img = Image.open(target_path).resize((1024, 1024))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img.thumbnail((1024, 1024))\n",
    "decor_prompt = \"Generate a prompt to add multiple furniture items to an empty room in the format: Add [a/an modifier furniture item orientation/location], [a/an modifier furniture item orientation/location], etc. Return prompt only.\"\n",
    "furnishing_prompt = gemini_cleaner.talk_with_gemini([img], decor_prompt)\n",
    "\n",
    "uniworld_img = run_inference(furnishing_prompt, img)[0]\n",
    "\n",
    "gemini_img = gemini_cleaner.clean_image(img, furnishing_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "\n",
    "def annotate(img, text, position=\"right\"):\n",
    "    annotated = img.copy()\n",
    "    draw = ImageDraw.Draw(annotated)\n",
    "    font_size = 40\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", font_size)\n",
    "    except IOError:\n",
    "        font = ImageFont.load_default(size=font_size)\n",
    "\n",
    "    # Calculate text size using textbbox\n",
    "    bbox = draw.textbbox((0, 0), text, font=font)\n",
    "    text_width = bbox[2] - bbox[0]\n",
    "    text_height = bbox[3] - bbox[1]\n",
    "\n",
    "    padding = 10\n",
    "    img_width, img_height = img.size\n",
    "\n",
    "    if position == \"right\":\n",
    "        x = img_width - text_width - 2 * padding\n",
    "    else:  # default to \"left\"\n",
    "        x = padding\n",
    "\n",
    "    y = padding\n",
    "\n",
    "    # Draw background rectangle\n",
    "    draw.rectangle(\n",
    "        [x - padding, y - padding, x + text_width + padding, y + text_height + padding],\n",
    "        fill=(0, 0, 0, 180),\n",
    "    )\n",
    "\n",
    "    # Draw text\n",
    "    draw.text((x, y), text, font=font, fill=(255, 255, 255))\n",
    "    return annotated\n",
    "\n",
    "\n",
    "annotate(img, \"restset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_image_grid([img, uniworld_img, gemini_img], cols=3, rows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(img_paths[0])\n",
    "furnishing_img_list = []\n",
    "\n",
    "\n",
    "for img_path in img_paths:\n",
    "    img = Image.open(img_path)\n",
    "    furnishing_prompt = gemini_cleaner.talk_with_gemini(\n",
    "        [img],\n",
    "        \"give me a image editing prompt for adding furnture to the given empty room, return prompt only, format Add furniture with description\",\n",
    "    )\n",
    "    furnishing_img = run_inference(furnishing_prompt, img)[0]\n",
    "    furnishing_img_list.append(furnishing_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_grid_list = []\n",
    "\n",
    "for i, image_path in enumerate(img_paths):\n",
    "    img = Image.open(image_path)\n",
    "    img.thumbnail((1024, 1024))\n",
    "    furnished_img = furnishing_img_list[i].resize(img.size)\n",
    "    img_grid = make_image_grid([img, furnished_img], cols=2, rows=1)\n",
    "    img_grid_list.append(img_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, img_grid in enumerate(img_grid_list):\n",
    "    img_grid.save(f\"/root/app/UniWorld-V1/comparison/{i}.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "univa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
