{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8fa5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    SiglipImageProcessor,\n",
    "    SiglipVisionModel,\n",
    "    T5EncoderModel,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from univa.models.qwen2p5vl.modeling_univa_qwen2p5vl import (\n",
    "    UnivaQwen2p5VLForConditionalGeneration,\n",
    ")\n",
    "from univa.utils.flux_pipeline import FluxPipeline\n",
    "from univa.utils.get_ocr import get_ocr_result\n",
    "from univa.utils.denoiser_prompt_embedding_flux import encode_prompt\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from univa.utils.anyres_util import dynamic_resize, concat_images_adaptive\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "import uuid\n",
    "import base64\n",
    "from typing import Dict\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from pathlib import Path\n",
    "from diffusers.utils import make_image_grid\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e143c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_siglip(siglip_path, device=\"cuda\"):\n",
    "    siglip_processor = SiglipImageProcessor.from_pretrained(siglip_path)\n",
    "    siglip_model = SiglipVisionModel.from_pretrained(\n",
    "        siglip_path,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    ).to(device)\n",
    "    return siglip_processor, siglip_model\n",
    "\n",
    "\n",
    "def load_lvlm(pretrained_lvlm_path, denoise_projector_path=None, siglip_projector_path=None, device=\"cuda\"):\n",
    "    lvlm_model = UnivaQwen2p5VLForConditionalGeneration.from_pretrained(\n",
    "        pretrained_lvlm_path,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "    ).to(device)\n",
    "\n",
    "    if denoise_projector_path:\n",
    "        denoise_projector = torch.load(denoise_projector_path, map_location=device)\n",
    "        msg = lvlm_model.load_state_dict(denoise_projector, strict=False)\n",
    "        assert len(msg[1]) == 0, f\"Missing keys in denoise projector: {msg[1]}\"\n",
    "\n",
    "    if siglip_projector_path:\n",
    "        siglip_projector = torch.load(siglip_projector_path, map_location=device)\n",
    "        msg = lvlm_model.load_state_dict(siglip_projector, strict=False)\n",
    "        assert len(msg[1]) == 0, f\"Missing keys in siglip projector: {msg[1]}\"\n",
    "\n",
    "    lvlm_processor = AutoProcessor.from_pretrained(\n",
    "        pretrained_lvlm_path,\n",
    "        min_pixels=1024 * 1024,\n",
    "        max_pixels=1024 * 1024,\n",
    "    )\n",
    "\n",
    "    return lvlm_processor, lvlm_model\n",
    "\n",
    "\n",
    "def load_flux(flux_path, lvlm_model, device=\"cuda\"):\n",
    "    return FluxPipeline.from_pretrained(\n",
    "        flux_path,\n",
    "        transformer=lvlm_model.denoise_tower.denoiser,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    ).to(device)\n",
    "\n",
    "\n",
    "def build_conversation(prompt, image, resolution=1024):\n",
    "    return [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": prompt},\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": image,\n",
    "                \"min_pixels\": resolution * resolution,\n",
    "                \"max_pixels\": resolution * resolution,\n",
    "            },\n",
    "        ],\n",
    "    }]\n",
    "\n",
    "\n",
    "def preprocess_lvlm_inputs(lvlm_processor, convo, image, device=\"cuda\"):\n",
    "    chat_text = lvlm_processor.apply_chat_template(\n",
    "        convo, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    chat_text = \"<|im_end|>\\n\".join(chat_text.split(\"<|im_end|>\\n\")[1:])\n",
    "\n",
    "    return lvlm_processor(\n",
    "        text=[chat_text],\n",
    "        images=[image],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "\n",
    "\n",
    "def get_siglip_hidden_states(siglip_processor, siglip_model, image, device=\"cuda\"):\n",
    "    tensor = siglip_processor.preprocess(\n",
    "        image.convert(\"RGB\"),\n",
    "        do_resize=True,\n",
    "        do_convert_rgb=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).pixel_values.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        return siglip_model(tensor).last_hidden_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8323e96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_img_and_clean_prompt(dataset_path: Path, idx: int):\n",
    "#     json_path = dataset_path / \"annotation.json\"\n",
    "\n",
    "\n",
    "#     with open(json_path, \"r\") as f:\n",
    "#         data = json.load(f)\n",
    "\n",
    "#     img_meta = data[idx]\n",
    "\n",
    "\n",
    "#     img_path = dataset_path / img_meta[\"image\"][0]\n",
    "\n",
    "#     src_img = Image.open(img_path)\n",
    "#     clean_prompt = img_meta[\"conversations\"][0][\"value\"]\n",
    "#     return src_img, clean_prompt\n",
    "\n",
    "\n",
    "def get_img_and_clean_prompt(dataset_path: Path):\n",
    "    img_paths = list(dataset_path.glob(\"*.jpg\"))\n",
    "\n",
    "    json_path = dataset_path / \"clean_prompt_meta.json\"\n",
    "\n",
    "    with open(json_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    img_prompt_pairs = []\n",
    "    for img_path in img_paths:\n",
    "        clean_prompt = data[img_path.name]\n",
    "        img = Image.open(img_path)\n",
    "        pair = (img,clean_prompt)\n",
    "        img_prompt_pairs.append(pair)\n",
    "    return img_prompt_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97435139",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LVLM_PATH = \"checkpoints/training-dataset.utils.aniweave.ai/uniworld-stage-2-cp-10000-1024_start_with_text_mpl_in_512/univa\"\n",
    "DENOISE_PROJECTOR_WEIGHT_PATH = \"checkpoints/denoise_projector.bin\"\n",
    "SIGLIP_PROJECTOR_WEIGHT_PATH = \"checkpoints/training-dataset.utils.aniweave.ai/uniworld-stage-2-cp-10000-1024_start_with_text_mpl_in_512/siglip_projector.bin\"\n",
    "\n",
    "SIGLIP_PATH = \"/workspace/UniWorld-V1/model_weight/siglip2-so400m-patch16-512\"\n",
    "FLUX_PATH = \"/workspace/UniWorld-V1/model_weight/FLUX.1-dev\"\n",
    "\n",
    "DATASET_FOLDER  = Path(\"/workspace/UniWorld-V1/xinyi_test_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2dae1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------- Part 1: Model Loading (Run Only Once) -------------------\n",
    "\n",
    "class ModelLoader:\n",
    "    def __init__(self, device=\"cuda\"):\n",
    "        self.device = device\n",
    "        self.siglip_processor = None\n",
    "        self.siglip_model = None\n",
    "        self.lvlm_processor = None\n",
    "        self.lvlm_model = None\n",
    "        self.pipe = None\n",
    "        self.empty_pooled_prompt_embeds = None\n",
    "\n",
    "    def load_all_models(self):\n",
    "        \"\"\"Loads all the necessary models onto the specified device.\"\"\"\n",
    "        print(\"Loading all models onto the GPU... This may take a moment.\")\n",
    "\n",
    "        # Load your models using your provided functions\n",
    "        self.siglip_processor, self.siglip_model = load_siglip(SIGLIP_PATH, device=self.device)\n",
    "        self.lvlm_processor, self.lvlm_model = load_lvlm(\n",
    "            LVLM_PATH,\n",
    "            DENOISE_PROJECTOR_WEIGHT_PATH,\n",
    "            SIGLIP_PROJECTOR_WEIGHT_PATH,\n",
    "            device=self.device,\n",
    "        )\n",
    "        self.pipe = load_flux(FLUX_PATH, self.lvlm_model, device=self.device)\n",
    "\n",
    "        # Pre-compute the empty prompt embeds once, as it's constant\n",
    "        self._precompute_empty_prompt()\n",
    "        print(\"Models loaded successfully.\")\n",
    "\n",
    "    def _precompute_empty_prompt(self):\n",
    "        \"\"\"Encodes an empty prompt for classifier-free guidance.\"\"\"\n",
    "        tokenizers = [self.pipe.tokenizer, self.pipe.tokenizer_2]\n",
    "        text_encoders = [self.pipe.text_encoder, self.pipe.text_encoder_2]\n",
    "\n",
    "        _, self.empty_pooled_prompt_embeds = encode_prompt(\n",
    "            text_encoders,\n",
    "            tokenizers,\n",
    "            prompt=\"\",\n",
    "            max_sequence_length=256,\n",
    "            device=self.device,\n",
    "            num_images_per_prompt=1,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dc99e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Initial Setup ---\n",
    "# Create an instance of the loader and load the models.\n",
    "# This is the section you only run one time.\n",
    "device = \"cuda\"\n",
    "model_manager = ModelLoader(device=device)\n",
    "model_manager.load_all_models()\n",
    "\n",
    "\n",
    "# ------------------- Part 2: Image Generation (Run as many times as you want) -------------------\n",
    "\n",
    "def generate_image(models, prompt, input_image, num_images_per_prompt=4, num_steps=30, guidance_scale=4.0):\n",
    "    \"\"\"\n",
    "    Generates an image using the pre-loaded models.\n",
    "    This function can be called repeatedly without reloading models.\n",
    "    \"\"\"\n",
    "    print(f\"Generating image for prompt: '{prompt}'\")\n",
    "    # Ensure image is in the correct format and size\n",
    "    processed_image = input_image.resize((1024, 1024))\n",
    "\n",
    "    # Build conversation\n",
    "    convo = build_conversation(prompt, processed_image)\n",
    "\n",
    "    # Encode inputs\n",
    "    lvlm_inputs = preprocess_lvlm_inputs(models.lvlm_processor, convo, processed_image, device=models.device)\n",
    "    siglip_hs = get_siglip_hidden_states(models.siglip_processor, models.siglip_model, processed_image, device=models.device)\n",
    "\n",
    "    # Forward pass to get embeddings\n",
    "    with torch.no_grad():\n",
    "        lvlm_embeds = models.lvlm_model(\n",
    "            lvlm_inputs.input_ids,\n",
    "            pixel_values=getattr(lvlm_inputs, \"pixel_values\", None),\n",
    "            attention_mask=lvlm_inputs.attention_mask,\n",
    "            image_grid_thw=getattr(lvlm_inputs, \"image_grid_thw\", None),\n",
    "            siglip_hidden_states=siglip_hs,\n",
    "            output_type=\"denoise_embeds\",\n",
    "        )\n",
    "\n",
    "    # Generate image using the pre-loaded diffusion pipe\n",
    "    generator = torch.Generator(device=models.device)\n",
    "    images = models.pipe(\n",
    "        prompt_embeds=lvlm_embeds,\n",
    "        pooled_prompt_embeds=models.empty_pooled_prompt_embeds, # Use the pre-computed empty embeds\n",
    "        height=1024,\n",
    "        width=1024,\n",
    "        num_inference_steps=num_steps,\n",
    "        guidance_scale=guidance_scale,\n",
    "        generator=generator,\n",
    "        num_images_per_prompt=num_images_per_prompt,\n",
    "    ).images\n",
    "    \n",
    "    # It's good practice to clear cache after a generation step\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(\"Image generation complete.\")\n",
    "    return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a188c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------- Example Usage -------------------\n",
    "\n",
    "# --- First Generation ---\n",
    "# In this cell, you can now call the generation function.\n",
    "\n",
    "\n",
    "\n",
    "img_clean_prompt_pairs = get_img_and_clean_prompt(DATASET_FOLDER)\n",
    "\n",
    "for i, (ori_img, prompt) in enumerate(img_clean_prompt_pairs):\n",
    "    cleaned_imgs = generate_image(model_manager, prompt, ori_img)\n",
    "    cleaned_imgs = [img.resize(ori_img.size) for img in cleaned_imgs]\n",
    "\n",
    "    img_list = [ori_img] + cleaned_imgs\n",
    "    img_grid = make_image_grid(img_list, cols=5, rows=1)\n",
    "    img_grid.save(f\"/workspace/UniWorld-V1/xinyi_test_data/1024_comparison/{i}.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658d5d38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "univa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
